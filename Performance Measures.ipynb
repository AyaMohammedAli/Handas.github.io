{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee37f3d-2ded-4ee4-8e30-b936f59d95da",
   "metadata": {},
   "source": [
    "## Classification:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbeba0-9e8c-4686-98a2-954365e7a1f2",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "\n",
    "- 1  <span style=\"background-color: Red;\">Measuring Accuracy Using Cross-Validation </span>\n",
    "\n",
    "  Measuring accuracy using cross-validation is a common practice in machine learning to assess the performance of a model on different subsets of the dataset. Cross-validation helps provide a more robust evaluation, particularly when the dataset is limited. Here are the general steps for measuring accuracy using cross-validation:\n",
    "  \n",
    "  Data Splitting:\n",
    "\n",
    "    Divide your dataset into k subsets or folds. Common choices for k are 5 or 10, but the value can vary depending on the dataset size and characteristics.\n",
    "    Training and Testing:\n",
    "\n",
    "    Perform training and testing k times, each time using a different fold as the test set and the remaining folds as the training set. This ensures that each data point is used for testing exactly once.\n",
    "    Model Training:\n",
    "\n",
    "    Train your machine learning model on the training set for each iteration of the cross-validation.\n",
    "    Model Evaluation:\n",
    "\n",
    "    Evaluate the model's performance on the corresponding test set for each iteration. This involves making predictions on the test set and comparing them to the actual labels.\n",
    "    Accumulate Results:\n",
    "\n",
    "    Accumulate the evaluation results (e.g., accuracy) from each iteration to get a comprehensive view of the model's performance across different subsets of the data.\n",
    "    Compute Average Accuracy:\n",
    "\n",
    "    Calculate the average accuracy across all iterations. This average accuracy provides a more reliable estimate of the model's generalization performance.\n",
    "    Interpretation:\n",
    "\n",
    "    Analyze the average accuracy along with any other relevant metrics to understand how well the model is likely to perform on new, unseen data.\n",
    "    \n",
    "- 2  <span style=\"background-color: red;\">Confusion metrix:</span>\n",
    " \n",
    "                   Predicted Class\n",
    "                    |  Positive    |  Negative    |\n",
    "      Actual Class  |------------ |------------  |\n",
    "      Positive     |    TP         |    FP      |\n",
    "      Negative     |    FN         |    TN      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05da00c-8cb1-42d7-96c1-8dc399668bc7",
   "metadata": {},
   "source": [
    "### Percision VS Recall:\n",
    "Imagine you have a basket filled with various fruits, and you want to build a classifier to identify apples from the mix. Precision and recall are two metrics that help evaluate the performance of your apple-detection model.\n",
    "\n",
    "Now, let's illustrate the concept:\n",
    "\n",
    "## Perfect Precision (but limited usefulness):\n",
    "You decide to achieve perfect precision by making only one prediction - you confidently declare that there is an apple in the basket, and indeed, it is correct. Your precision is now 100% (1/1 = 100%). However, this approach is not very practical, as it completely ignores all the other fruits in the basket. Your classifier is overly cautious and only identifies one positive instance (an apple) at the expense of missing other potential apples.\n",
    "\n",
    "\\begin{equation}\n",
    " \\text{Precision} = \\frac{TP}{TP + FP} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Introduction of Recall:\n",
    "Recognizing the limitation of focusing solely on precision, you decide to consider another metric - recall. <span style=\"background-color: green;\">Recall measures how many actual positive instances your classifier can identify correctly.</span> So, in addition to precision, you want to ensure that your classifier doesn't miss any apples.\n",
    "\n",
    "To improve recall, you need to broaden your approach. Now, you carefully examine the entire basket and make predictions for every potential apple. Some predictions may be incorrect (false positives), but the goal is to capture as many actual apples as possible. This balance between precision and recall ensures that your classifier is not overly conservative, and it actively seeks out positive instances.\n",
    "\\begin{equation}\n",
    " \\text{Precision} = \\frac{TP+FN}{TP } \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "So, while perfect precision might involve making just one correct prediction, the interplay between precision and recall encourages a more balanced and useful classifier. It aims to identify most of the actual positive instances while minimizing false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f860aa1-eb89-4324-96f0-c984ad4beca6",
   "metadata": {},
   "source": [
    "It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall . Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high.\n",
    "\\begin{equation}\n",
    "\\text{F1-Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd47de-ba02-4edc-b933-522fc2ff98fd",
   "metadata": {},
   "source": [
    "> **Note:** If someone says, “Let’s reach 99% precision”, you should ask, “At what recall?”\n",
    "   It's common for precision and recall to have an inverse relationship. As precision increases, recall may decrease, and vice versa. This is due to the fact that increasing the   threshold for positive predictions tends to reduce false positives but may also result in more false negatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac67bb-3bd5-4537-a5fd-6b2137ac0462",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### The Receiver Operating Characteristic (ROC) curve:\n",
    "    is a graphical representation of a binary classification model's performance across various discrimination thresholds. It illustrates the trade-off between the true positive rate (sensitivity or recall) and the false positive rate as the classification threshold is varied.\n",
    "    \n",
    "    ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62134a74-7381-4253-8e65-8130995fb5d2",
   "metadata": {},
   "source": [
    "![Example Image](RocCurve.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2354f8-bb76-4726-b417-d7cbda2e2d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e42540-eaa3-484e-b1ff-e2c56050d306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
